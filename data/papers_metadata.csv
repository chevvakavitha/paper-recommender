id,title,abstract,authors,year,pdf_link
1,Attention Is All You Need,We propose a new simple network architecture called the Transformer based on an attention mechanism. The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture based solely on attention mechanisms dispensing with recurrence and convolutions entirely.,Vaswani et al.,2017,https://arxiv.org/pdf/1706.03762.pdf
2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,We introduce BERT a method of pre-training language representations. BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks.,Devlin et al.,2018,https://arxiv.org/pdf/1810.04805.pdf
3,GPT-3: Language Models are Few-Shot Learners,Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a diverse corpus of text followed by fine-tuning on a specific task. We demonstrate that scaling up language models greatly improves task-agnostic few-shot performance.,Brown et al.,2020,https://arxiv.org/pdf/2005.14165.pdf
4,A Survey on Deep Learning for Time Series Forecasting,Deep learning methods have proven to be highly effective for time series forecasting. We provide a comprehensive overview of deep learning techniques and their applications to forecasting various types of time series.,Zhang et al.,2020,https://arxiv.org/pdf/2004.13408.pdf
5,Transformers for Time Series: A Survey,This survey comprehensively reviews Transformer models applied to time series analysis tasks including forecasting classification and anomaly detection.,Wen et al.,2023,https://arxiv.org/pdf/2205.13504.pdf
6,ResNet: Deep Residual Learning for Image Recognition,Deep residual learning framework eases the training of very deep networks. We present residual networks that are substantially deeper than previously feasible and obtain remarkable accuracy on challenging image recognition tasks.,He et al.,2015,https://arxiv.org/pdf/1512.03385.pdf
7,Convolutional Neural Networks for Image Classification,We investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting using an architecture with small 3x3 convolution filters.,Simonyan et al.,2014,https://arxiv.org/pdf/1409.1556.pdf
8,YOLO: You Only Look Once - Real-time Object Detection,You Only Look Once provides a unified real-time framework for object detection. Prior detection systems repurpose classifiers to perform detection. Our system frames object detection as a regression problem to spatially separated bounding boxes.,Redmon et al.,2015,https://arxiv.org/pdf/1506.02640.pdf
9,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,We introduce Region Proposal Networks to enable end-to-end trainable region-based fully convolutional networks for object detection.,Ren et al.,2015,https://arxiv.org/pdf/1506.01497.pdf
10,Mask R-CNN,We present an instance segmentation framework that extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition.,He et al.,2017,https://arxiv.org/pdf/1703.06870.pdf
11,Vision Transformers: An Image is Worth 16x16 Words,While the Transformer architecture has become the de-facto standard for natural language processing tasks the applications to computer vision remain limited. We show that a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks.,Dosovitskiy et al.,2020,https://arxiv.org/pdf/2010.11929.pdf
12,CLIP: Learning Transferable Models For Zero-Shot Learning,We study how the ability to describe images using natural language enables zero-shot transfer. We present the CLIP model trained on image-text pairs from the internet.,Radford et al.,2021,https://arxiv.org/pdf/2103.14030.pdf
13,Generative Adversarial Nets,We propose a new framework for estimating generative models via an adversarial process where we simultaneously train two models: a generative model G that captures the data distribution and a discriminative model D that estimates the probability that a sample came from the training data.,Goodfellow et al.,2014,https://arxiv.org/pdf/1406.2661.pdf
14,Variational Autoencoders,We introduce a stochastic variational inference and learning algorithm that scales to large datasets and is easily differentiable through standard integrational gradient methods.,Kingma et al.,2013,https://arxiv.org/pdf/1312.6114.pdf
15,Diffusion Models for Image Generation,Diffusion probabilistic models are learned via a straightforward weighted variational lower bound on negative log likelihood. Our best models outperform PixelCNN++ and achieve state-of-the-art likelihood scores.,Ho et al.,2020,https://arxiv.org/pdf/2006.11239.pdf
16,LSTM: Long Short-Term Memory,We propose a novel recurrent network architecture in response to the limitations of conventional RNNs in learning long-term dependencies.,Hochreiter et al.,1997,https://arxiv.org/pdf/1211.5063.pdf
17,GRU: Learning Phrase Representations using RNN Encoder-Decoder,We propose a novel neural network model called RNN Encoder-Decoder with gated hidden units. The proposed architecture enables a model to automatically search for parts of a source sentence that are relevant to predicting a target word.,Cho et al.,2014,https://arxiv.org/pdf/1406.1078.pdf
18,RoBERTa: A Robustly Optimized BERT Pretraining Approach,We present an improved training methodology for BERT pretraining that leads to improved performance on downstream tasks.,Liu et al.,2019,https://arxiv.org/pdf/1907.11692.pdf
19,ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators,We present ELECTRA a new pretraining approach which trains two neural networks in a new way inspired by generative adversarial networks.,Clark et al.,2020,https://arxiv.org/pdf/2003.10555.pdf
20,T5: Text-to-Text Transfer Transformer,We introduce a unified framework that converts all NLP problems into a text-to-text format.,Raffel et al.,2019,https://arxiv.org/pdf/1910.10683.pdf
21,Seq2Seq with Attention,Sequence-to-sequence learning with neural networks has emerged as a promising new framework for machine translation. The encoder-decoder architecture with attention mechanism significantly improves performance.,Bahdanau et al.,2014,https://arxiv.org/pdf/1409.0473.pdf
22,Neural Machine Translation by Jointly Learning to Align and Translate,A new approach to machine translation is proposed which consists of an encoder and a decoder that reads an input sequence and generates an output sequence.,Cho et al.,2014,https://arxiv.org/pdf/1409.7268.pdf
23,Word2Vec: Efficient Estimation of Word Representations in Vector Space,We propose two novel model architectures for learning word embeddings from very large data sets. The resulting word vectors are shown to carry semantic and syntactic word relationships useful for many NLP applications.,Mikolov et al.,2013,https://arxiv.org/pdf/1301.3781.pdf
24,GloVe: Global Vectors for Word Representation,We present GloVe an unsupervised learning algorithm for obtaining vector representations for words. The resulting representations exhibit an interesting linear structure of the word vector space.,Pennington et al.,2014,https://arxiv.org/pdf/1504.06654.pdf
25,Distributed Representations of Sentences and Documents,We present Paragraph Vector a method for unsupervised learning of continuous representations of paragraphs and documents.,Le et al.,2014,https://arxiv.org/pdf/1405.4053.pdf
26,FastText: Enriching Word Vectors with Subword Information,We propose FastText an extension of the Word2Vec model that learns representations for character n-grams and sums up these representations to obtain word vectors.,Bojanowski et al.,2016,https://arxiv.org/pdf/1607.04606.pdf
27,ELMo: Deep contextualized word representations,We introduce a new type of deep contextualized word representation that models both complex characteristics of word use as well as how these uses vary across linguistic contexts.,Peters et al.,2018,https://arxiv.org/pdf/1802.05365.pdf
28,XLNet: Generalized Autoregressive Pretraining for Language Understanding,We present XLNet an autoregressive pretraining method that outperforms BERT on multiple benchmarks.,Yang et al.,2019,https://arxiv.org/pdf/1906.08237.pdf
29,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,We present ALBERT which stands for A Lite BERT. We show that ALBERT outperforms BERT and previous state-of-the-art models on various benchmarks.,Lan et al.,2019,https://arxiv.org/pdf/1909.11942.pdf
30,Dropout: A Simple Way to Prevent Neural Networks from Overfitting,Dropout is a technique for addressing overfitting in neural networks by probabilistically dropping units from the network during training.,Hinton et al.,2012,https://arxiv.org/pdf/1207.0580.pdf
31,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,We introduce a simple yet effective technique to accelerate deep network training by reducing internal covariate shift.,Ioffe et al.,2015,https://arxiv.org/pdf/1502.03167.pdf
32,Layer Normalization,We introduce Layer Normalization a simple reparameterization of the network that decouples the training of a model from its weight initialization.,Ba et al.,2016,https://arxiv.org/pdf/1607.06450.pdf
33,Adam: A Method for Stochastic Optimization,We introduce Adam an algorithm for first-order gradient-based optimization of stochastic objective functions with adaptive learning rates.,Kingma et al.,2014,https://arxiv.org/pdf/1412.6980.pdf
34,Neural Architecture Search with Reinforcement Learning,We introduce a method to design neural network architectures using a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the accuracy.,Zoph et al.,2016,https://arxiv.org/pdf/1611.01578.pdf
35,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,We systematically study model scaling and propose a compound scaling method that uniformly scales all dimensions of depth width and resolution.,Tan et al.,2019,https://arxiv.org/pdf/1905.11946.pdf
36,MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,We present MobileNets efficient convolutional neural networks for mobile and resource constrained vision applications.,Howard et al.,2017,https://arxiv.org/pdf/1704.04861.pdf
37,SqueezeNet: AlexNet-level accuracy with 50x fewer parameters,We present SqueezeNet a small deep convolutional neural network that achieves AlexNet-level accuracy with 50x fewer parameters.,Iandola et al.,2016,https://arxiv.org/pdf/1602.07360.pdf
38,DenseNet: Densely Connected Convolutional Networks,We introduce DenseNet a convolutional network where each layer is connected to every other layer in a feed-forward fashion.,Huang et al.,2016,https://arxiv.org/pdf/1608.06993.pdf
39,Inception: Going Deeper with Convolutions,We propose a deep convolutional neural network architecture codenamed Inception which was responsible for setting the new state of the art for classification and detection in the ImageNet dataset.,Szegedy et al.,2014,https://arxiv.org/pdf/1409.4842.pdf
40,Transfer Learning and Domain Adaptation,Transfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task.,Yosinski et al.,2014,https://arxiv.org/pdf/1411.1792.pdf
41,Meta-Learning: Learning to Learn,We introduce a meta-learning approach to enable neural networks to learn how to learn new tasks from a small amount of data.,Finn et al.,2017,https://arxiv.org/pdf/1703.03400.pdf
42,Few-Shot Learning with Siamese Neural Networks,We explore matching networks for one-shot learning. Our suggested approach is based on a strategy of adding an attention mechanism to a siamese neural network.,Vinyals et al.,2016,https://arxiv.org/pdf/1606.04080.pdf
43,Prototypical Networks for Few-shot Learning,We propose prototypical networks for the problem of few-shot classification. The network learns a metric space in which classification can be performed by computing distances to prototype representations.,Snell et al.,2017,https://arxiv.org/pdf/1703.05175.pdf
44,Relation Networks for Object Detection,Simple models can outperform more complex ones. We propose relation networks which is the first model that combines visual features and spatial relations.,Hu et al.,2018,https://arxiv.org/pdf/1711.11575.pdf
45,Capsule Networks: Novel Deep Learning Architecture,We present a new type of neural net element called a capsule that automatically installs equivariance and more. All the neural network capsules are merged into a single prediction with routing-by-agreement.,Sabour et al.,2017,https://arxiv.org/pdf/1710.09829.pdf
46,Graph Neural Networks: A Review of Methods and Applications,This article provides a comprehensive review of graph neural networks covering various models and applications.,Wu et al.,2020,https://arxiv.org/pdf/1901.00596.pdf
47,Attention Networks Revisited,We revisit the transformer architecture and provide new insights into its effectiveness for various sequence-to-sequence tasks.,Bahdanau et al.,2017,https://arxiv.org/pdf/1706.03762.pdf
48,RNN and LSTM Evolution,Long Short-Term Memory networks are an extension of recurrent neural networks that can learn long-term dependencies.,Schmidhuber et al.,2001,https://arxiv.org/pdf/1207.0338.pdf
49,Anomaly Detection in Time Series Data,We present methods for detecting anomalies in time series data using deep learning approaches including autoencoders and LSTM networks.,Zhang et al.,2019,https://arxiv.org/pdf/1903.03039.pdf
50,Reinforcement Learning: An Introduction,Reinforcement learning is a machine learning paradigm for learning sequential decision making tasks through interaction with the environment.,Sutton et al.,2018,https://arxiv.org/pdf/1711.02063.pdf
51,Deep Q-Networks for Game Playing,We present deep Q-networks which combine Q-learning with deep neural networks to learn control policies directly from high-dimensional sensory input.,Mnih et al.,2013,https://arxiv.org/pdf/1312.5602.pdf
52,Policy Gradient Methods in Reinforcement Learning,We present policy gradient methods for solving reinforcement learning problems which directly parameterize the policy and update parameters by gradient descent.,Williams et al.,1992,https://arxiv.org/pdf/1812.11965.pdf
53,Actor-Critic Methods: Advantage Functions,We present actor-critic methods which combine policy gradient with value functions for more stable reinforcement learning.,Konda et al.,2000,https://arxiv.org/pdf/1912.04977.pdf
54,Knowledge Distillation: Transferring Knowledge from Large to Small Models,We explore methods for compressing neural networks through knowledge distillation where a small model is trained to mimic a larger model.,Hinton et al.,2015,https://arxiv.org/pdf/1503.02531.pdf
55,Contextual Embeddings for NLP,We introduce methods for learning contextual embeddings that capture semantic meaning of words based on surrounding context.,Devlin et al.,2018,https://arxiv.org/pdf/1802.05365.pdf
